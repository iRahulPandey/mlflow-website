---
title: MLflow 1.20.0
slug: 1.20.0
authors: [mlflow-maintainers]
---

We are happy to announce the availability of [MLflow 1.20.0](https://github.com/mlflow/mlflow/releases/tag/v1.20.0)!

Note: The MLflow R package for 1.20.0 is not yet available but will be in a week because CRAN's submission system will be offline until September 1st.

In addition to bug and documentation fixes, MLflow 1.20.0 includes the following features and improvements:

- Autologging for scikit-learn now records post training metrics when scikit-learn evaluation APIs, such as `sklearn.metrics.mean_squared_error`, are called ([#4491](https://github.com/mlflow/mlflow/pull/4491), [#4628](https://github.com/mlflow/mlflow/pull/4628) [#4638](https://github.com/mlflow/mlflow/pull/4638), [@WeichenXu123](https://github.com/WeichenXu123))
- Autologging for PySpark ML now records post training metrics when model evaluation APIs, such as `Evaluator.evaluate()`, are called ([#4686](https://github.com/mlflow/mlflow/pull/4686), [@WeichenXu123](https://github.com/WeichenXu123))
- Add `pip_requirements` and `extra_pip_requirements` to `mlflow.*.log_model` and `mlflow.*.save_model` for directly specifying the pip requirements of the model to log / save ([#4519](https://github.com/mlflow/mlflow/pull/4519), [#4577](https://github.com/mlflow/mlflow/pull/4577), [#4602](https://github.com/mlflow/mlflow/pull/4602), [@harupy](https://github.com/harupy))
- Added `stdMetrics` entries to the training metrics recorded during PySpark CrossValidator autologging ([#4672](https://github.com/mlflow/mlflow/pull/4672), [@WeichenXu123](https://github.com/WeichenXu123))
- MLflow UI updates:
  1. Improved scalability of the parallel coordinates plot for run performance comparison,
  2. Added support for filtering runs based on their start time on the experiment page,
  3. Added a dropdown for runs table column sorting on the experiment page,
  4. Upgraded the AG Grid plugin, which is used for runs table loading on the experiment page, to version 25.0.0,
  5. Fixed a bug on the experiment page that caused the metrics section of the runs table to collapse when selecting columns from other table sections ([#4712](https://github.com/mlflow/mlflow/pull/4712), [@dbczumar](https://github.com/dbczumar))
- Added support for distributed execution to autologging for PyTorch Lightning ([#4717](https://github.com/mlflow/mlflow/pull/4717), [@dbczumar](https://github.com/dbczumar))
- Expanded R support for Model Registry functionality ([#4527](https://github.com/mlflow/mlflow/pull/4527), [@bramrodenburg](https://github.com/bramrodenburg))
- Added model scoring server support for defining custom prediction response wrappers ([#4611](https://github.com/mlflow/mlflow/pull/4611), [@Ark-kun](https://github.com/Ark-kun))
- `mlflow.*.log_model` and `mlflow.*.save_model` now automatically infer the pip requirements of the model to log / save based on the current software environment ([#4518](https://github.com/mlflow/mlflow/pull/4518), [@harupy](https://github.com/harupy))
- Introduced support for running Sagemaker Batch Transform jobs with MLflow Models ([#4410](https://github.com/mlflow/mlflow/pull/4410), [#4589](https://github.com/mlflow/mlflow/pull/4589), [@YQ-Wang](https://github.com/YQ-Wang))

For a comprehensive list of changes, see the [release change log](https://github.com/mlflow/mlflow/releases/tag/v1.20.0), and check out the latest documentation on [mlflow.org](http://mlflow.org/).
