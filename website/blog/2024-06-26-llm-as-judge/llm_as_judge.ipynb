{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging LLMs as Judges: Advanced Model Evaluation with MLflow\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In the rapidly evolving field of machine learning, evaluating model performance is crucial. MLflow, an open-source platform for managing the ML lifecycle, offers powerful tools for assessing models comprehensively. In this blog post, we'll explore how to use MLflow evaluate, with a special focus on leveraging Large Language Models (LLMs) as judges for custom evaluation metrics.\n",
    "\n",
    "## 2. What is MLflow evaluate?\n",
    "\n",
    "MLflow evaluate is a function that enables you to evaluate machine learning models using various metrics. It supports both built-in metrics and custom metrics, allowing for flexible and thorough model assessment. \n",
    "\n",
    "With MLflow evaluate, you can:\n",
    "\n",
    "- Evaluate models against multiple metrics simultaneously\n",
    "- Use pre-defined metrics for specific model types (e.g., question-answering, text-summarization)\n",
    "- Create custom metrics, including those that use LLMs as judges\n",
    "\n",
    "MLflow LLM Evaluate is a functionality provided by MLflow to assess the performance of Large Language Models (LLMs). It consists of three main components:\n",
    "\n",
    "- A `model` to evaluate: This can be an MLflow pyfunc model, a URI pointing to a registered MLflow model, or any Python callable representing the model. If you already computed your output then you can re-run multiple evaluations on an already existing LLM output.\n",
    "- `Metrics`: These are used to compute the performance of the model. MLflow uses LLM-specific metrics for evaluation.\n",
    "- Evaluation `data`: This is the data used to evaluate the model, which can be in various formats like pandas DataFrame, Python list, numpy array, or an mlflow.data.dataset.Dataset() instance.\n",
    "\n",
    "## 3. Why use LLMs as judges?\n",
    "\n",
    "Traditional metrics often fall short when evaluating complex language tasks. LLMs, with their advanced language understanding capabilities, can act as impartial judges, providing nuanced assessments of model outputs. This is particularly useful for tasks where human-like judgment is beneficial, such as assessing the quality of language translations, the coherence of generated text, or the appropriateness of responses in conversational AI.\n",
    "\n",
    "In this blog we will focus on use cases where we don't have ground truth avaiable do compare our model's output to.\n",
    "\n",
    "\n",
    "### Industry example:\n",
    "\n",
    "Companies looking to enhance the professionalism of their customer-facing chatbots can integrate LLMs into their evaluation frameworks. By assessing `professionalism` with attributes such as politeness, clarity, and empathy using custom metrics in MLflow, organizations can fine-tune chatbot responses based on LLM assessments. This approach can lead to improved customer satisfaction and overall interaction quality, contributing to a more positive user experience.\n",
    "\n",
    "\n",
    "## 4. GenAI Metrics\n",
    "\n",
    "MLflow offers a few pre-canned metrics which uses LLM as the judge. Despite the difference under the hood, the usage is the same - put these metrics in the extra_metrics argument in mlflow.evaluate(). Here is the list of pre-canned metrics:\n",
    "\n",
    "1. **mlflow.metrics.genai.answer_similarity()**\n",
    "   - **Purpose**: Evaluates semantic similarity between model output and ground truth.\n",
    "   - **Use case**: Ideal for question-answering tasks.\n",
    "   - **Scoring**: Higher scores indicate greater alignment.\n",
    "\n",
    "2. **mlflow.metrics.genai.answer_correctness()**\n",
    "   - **Purpose**: Assesses factual correctness based on ground truth.\n",
    "   - **Use case**: Crucial for knowledge-based question answering.\n",
    "   - **Scoring**: Higher scores indicate similarity and factual correctness.\n",
    "\n",
    "3. **mlflow.metrics.genai.answer_relevance()**\n",
    "   - **Purpose**: Evaluates output relevance to the input question (ignoring context).\n",
    "   - **Use case**: Assesses if the model stays on topic.\n",
    "   - **Scoring**: Higher scores mean more topical relevance.\n",
    "\n",
    "4. **mlflow.metrics.genai.relevance()**\n",
    "   - **Purpose**: Measures relevance to both input question and context.\n",
    "   - **Use case**: Useful for question answering over specific contexts.\n",
    "   - **Scoring**: Higher scores indicate understanding of question and context.\n",
    "\n",
    "5. **mlflow.metrics.genai.faithfulness()**\n",
    "   - **Purpose**: Evaluates output faithfulness to provided context (ignoring question).\n",
    "   - **Use case**: Important for summarization or information extraction.\n",
    "   - **Scoring**: Higher scores mean better alignment with context.\n",
    "\n",
    "## 4. Custom metrics with LLM as judge\n",
    "\n",
    "MLflow provides two main functions for creating custom metrics using LLMs as judges:\n",
    "\n",
    "1. `mlflow.metrics.genai.make_genai_metric()`\n",
    "2. `mlflow.metrics.genai.make_genai_metric_from_prompt()`\n",
    "\n",
    "Let's explore both of these functions and their use cases.\n",
    "\n",
    "### 4.1 Using make_genai_metric()\n",
    "\n",
    "The `make_genai_metric()` function allows you to create a custom metric with more structure and guidance for the LLM judge. You can specify:\n",
    "\n",
    "- The metric name and definition\n",
    "- A grading prompt to guide the LLM judge\n",
    "- Example evaluations for calibration\n",
    "- The LLM model to use as a judge (e.g., GPT-4)\n",
    "\n",
    "Here's an example of creating a custom metric for cultural sensitivity in translations:\n",
    "\n",
    "```python\n",
    "cultural_sensitivity = mlflow.metrics.genai.make_genai_metric(\n",
    "    name=\"cultural_sensitivity\",\n",
    "    definition=\"Assesses how well the translation preserves cultural nuances and idioms.\",\n",
    "    grading_prompt=\"Score from 1-5, where 1 is culturally insensitive and 5 is highly culturally aware.\",\n",
    "    examples=[\n",
    "        mlflow.metrics.genai.EvaluationExample(\n",
    "            input=\"Break a leg!\",\n",
    "            output=\"¡Rómpete una pierna!\",\n",
    "            score=2,\n",
    "            justification=\"This is a literal translation that doesn't capture the idiomatic meaning.\"\n",
    "        ),\n",
    "        mlflow.metrics.genai.EvaluationExample(\n",
    "            input=\"Break a leg!\",\n",
    "            output=\"¡Mucha mierda!\",\n",
    "            score=5,\n",
    "            justification=\"This translation uses the equivalent Spanish theater idiom, showing high cultural awareness.\"\n",
    "        )\n",
    "    ],\n",
    "    model=\"openai:/gpt-4\",\n",
    "    parameters={\"temperature\": 0.0},\n",
    ")\n",
    "```\n",
    "\n",
    "### 4.2 Using make_genai_metric_from_prompt()\n",
    "\n",
    "The `make_genai_metric_from_prompt()` function provides more flexibility by allowing you to define the entire prompt for the LLM judge. This can be useful for use cases that are not covered by the full grading prompt in any `EvaluationModel` version. Here's the function signature:\n",
    "\n",
    "```python\n",
    "mlflow.metrics.genai.make_genai_metric_from_prompt(\n",
    "    name: str,\n",
    "    judge_prompt: Optional[str] = None,\n",
    "    model: Optional[str] = 'openai:/gpt-4',\n",
    "    parameters: Optional[Dict[str, Any]] = None,\n",
    "    aggregations: Optional[List[str]] = ['mean', 'variance', 'p90'],\n",
    "    greater_is_better: bool = True,\n",
    "    max_workers: int = 10,\n",
    "    metric_metadata: Optional[Dict[str, Any]] = None\n",
    ") → mlflow.models.evaluation.base.EvaluationMetric\n",
    "```\n",
    "\n",
    "Here's an example of creating a custom metric for ease of understanding using this function:\n",
    "\n",
    "```python\n",
    "from mlflow.metrics.genai import make_genai_metric_from_prompt\n",
    "\n",
    "ease_of_understanding = make_genai_metric_from_prompt(\n",
    "    name=\"ease_of_understanding\",\n",
    "    judge_prompt=(\n",
    "        \"You must evaluate the output of a bot based on how easy it is to \"\n",
    "        \"understand its outputs. \"\n",
    "        \"Evaluate the bot's output from the perspective of a layperson. \"\n",
    "        \"The bot was provided with this input: {input} and this output: {output}. \"\n",
    "        \"Rate the ease of understanding on a scale from 1 to 5, where 1 is very difficult \"\n",
    "        \"to understand and 5 is very easy to understand. \"\n",
    "        \"Provide your rating as a single number between 1 and 5.\"\n",
    "    ),\n",
    "    model=\"openai:/gpt-4\",\n",
    "    parameters={\"temperature\": 0.0},\n",
    "    aggregations=[\"mean\", \"variance\", \"p90\"],\n",
    "    greater_is_better=True,\n",
    ")\n",
    "```\n",
    "\n",
    "This approach gives you more control over the exact instructions given to the LLM judge, allowing for more specialized evaluation criteria.\n",
    "\n",
    "# 6. Selecting the LLMs \n",
    "\n",
    "In order to evaluate your LLM with mlflow.evaluate(), your LLM has to be one of the following type:\n",
    "1.\tA `mlflow.pyfunc.PyFuncModel()` instance or a URI pointing to a logged mlflow.pyfunc.PyFuncModel model. In general we call that MLflow model. The\n",
    "2.\tA `python function` that takes in string inputs and outputs a single string. \n",
    "3.\tAn `MLflow Deployments` endpoint URI pointing to a local MLflow Deployments Server, Databricks Foundation Models API, and External Models in Databricks Model Serving.\n",
    "4.\tSet `model=None`, and put model outputs in data. Only applicable when the data is a Pandas dataframe.\n",
    "\n",
    "# 7. Selecting the judge\n",
    "By default MLflow evaluate uses openai:/gpt-4 as a judge. However you can choose also choose a local model to do this evaluation (for example using ollama wraped in a pyfync)\n",
    "\n",
    "```python\n",
    "To use an endpoint hosted by a local MLflow Deployments Server, you can use the following code.\n",
    "from mlflow.deployments import set_deployments_target\n",
    "\n",
    "set_deployments_target(\"http://localhost:5000\")\n",
    "my_answer_similarity = mlflow.metrics.genai.answer_similarity(\n",
    "    model=\"endpoints:/my-endpoint\"\n",
    ")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Example Use case \n",
    "\n",
    "In this notebook we will be doing a simplified version of a mock industry use case.\n",
    "\n",
    "\n",
    "## Industry Example: Enhancing Translation Quality Using LLMs\n",
    "\n",
    "### Example: Improving Translation Accuracy and Cultural Appropriateness\n",
    "\n",
    "A travel agency aimed to improve the accuracy and cultural appropriateness of their translations to effectively communicate with international clients. They integrated LLMs into their evaluation framework to ensure translations captured cultural nuances and conveyed messages accurately across languages.\n",
    "\n",
    "### Case Study:\n",
    "\n",
    "#### Objective:\n",
    "\n",
    "The objective was to enhance the quality of translations to provide accurate and culturally appropriate content for international clients, ensuring clarity and context-specific accuracy.\n",
    "\n",
    "#### Methodology:\n",
    "\n",
    "1. **Data Collection:**\n",
    "   - The travel agency curated a diverse dataset of travel-related content, including promotional materials and customer communications, in multiple languages.\n",
    "\n",
    "2. **Custom Metrics:**\n",
    "   - Custom evaluation metrics were developed using the LLM. These metrics included scores for translation accuracy, cultural appropriateness, and naturalness.\n",
    "\n",
    "#### Implementation:\n",
    "\n",
    "1. **MLflow Integration:**\n",
    "   - The custom metrics were integrated into the MLflow evaluation framework. This facilitated continuous monitoring and improvement of translation quality across different languages.\n",
    "\n",
    "2. **Feedback Loop:**\n",
    "   - Evaluation results from the LLM were utilized iteratively to refine translation algorithms and ensure consistent quality and accuracy in communications.\n",
    "\n",
    "#### Results:\n",
    "\n",
    "1. **Cultural Sensitivity:**\n",
    "   - The LLM identified translations that required adjustments to better align with cultural norms and preferences, enhancing the agency's messaging for diverse audiences.\n",
    "\n",
    "2. **Contextual Accuracy:**\n",
    "   - By leveraging LLM-based contextual analysis, the travel agency ensured that translations accurately conveyed intended meanings in various travel contexts.\n",
    "\n",
    "3. **Quality Assurance:**\n",
    "   - Integration of LLMs provided robust quality assurance for translations, reducing errors and enhancing communication effectiveness with international clientele.\n",
    "\n",
    "#### Outcome:\n",
    "\n",
    "Integrating LLMs into their translation evaluation process significantly elevated the travel agency's ability to deliver accurate, culturally appropriate content to international clients. This improvement not only strengthened client communication but also enhanced the agency's reputation for providing high-quality service across global markets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow==2.14.1 in ./.venv/lib/python3.10/site-packages (2.14.1)\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.10/site-packages (1.35.3)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.10/site-packages (0.18.1)\n",
      "Requirement already satisfied: evaluate in ./.venv/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: tiktoken in ./.venv/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: fastapi in ./.venv/lib/python3.10/site-packages (0.111.0)\n",
      "Requirement already satisfied: rouge_score in ./.venv/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: textstat in ./.venv/lib/python3.10/site-packages (0.7.3)\n",
      "Requirement already satisfied: tenacity in ./.venv/lib/python3.10/site-packages (8.4.2)\n",
      "Requirement already satisfied: plotly in ./.venv/lib/python3.10/site-packages (5.22.0)\n",
      "Requirement already satisfied: ipykernel in ./.venv/lib/python3.10/site-packages (6.29.4)\n",
      "Requirement already satisfied: nbformat==5.10.4 in ./.venv/lib/python3.10/site-packages (5.10.4)\n",
      "Requirement already satisfied: Flask<4 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (3.0.3)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (1.13.1)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (3.0.0)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (7.1.0)\n",
      "Requirement already satisfied: entrypoints<1 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (3.1.43)\n",
      "Requirement already satisfied: graphene<4 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (3.3)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (7.1.0)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (3.6)\n",
      "Requirement already satisfied: matplotlib<4 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (3.9.0)\n",
      "Requirement already satisfied: numpy<2 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (1.26.4)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (1.25.0)\n",
      "Requirement already satisfied: packaging<25 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (24.1)\n",
      "Requirement already satisfied: pandas<3 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (2.2.2)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (4.25.3)\n",
      "Requirement already satisfied: pyarrow<16,>=4.0.0 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (15.0.2)\n",
      "Requirement already satisfied: pytz<2025 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (2024.1)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (6.0.1)\n",
      "Requirement already satisfied: querystring-parser<2 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (1.2.4)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (2.32.3)\n",
      "Requirement already satisfied: scikit-learn<2 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (1.5.0)\n",
      "Requirement already satisfied: scipy<2 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (1.14.0)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (2.0.31)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (0.5.0)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (3.1.4)\n",
      "Requirement already satisfied: gunicorn<23 in ./.venv/lib/python3.10/site-packages (from mlflow==2.14.1) (22.0.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in ./.venv/lib/python3.10/site-packages (from nbformat==5.10.4) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in ./.venv/lib/python3.10/site-packages (from nbformat==5.10.4) (4.22.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.venv/lib/python3.10/site-packages (from nbformat==5.10.4) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in ./.venv/lib/python3.10/site-packages (from nbformat==5.10.4) (5.14.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.10/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.10/site-packages (from openai) (2.7.4)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.10/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: dill in ./.venv/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.venv/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in ./.venv/lib/python3.10/site-packages (from fastapi) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in ./.venv/lib/python3.10/site-packages (from fastapi) (0.0.4)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in ./.venv/lib/python3.10/site-packages (from fastapi) (0.0.9)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in ./.venv/lib/python3.10/site-packages (from fastapi) (5.10.0)\n",
      "Requirement already satisfied: orjson>=3.2.1 in ./.venv/lib/python3.10/site-packages (from fastapi) (3.10.5)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in ./.venv/lib/python3.10/site-packages (from fastapi) (2.2.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (0.30.1)\n",
      "Requirement already satisfied: absl-py in ./.venv/lib/python3.10/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.10/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in ./.venv/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: pyphen in ./.venv/lib/python3.10/site-packages (from textstat) (0.15.0)\n",
      "Requirement already satisfied: appnope in ./.venv/lib/python3.10/site-packages (from ipykernel) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./.venv/lib/python3.10/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./.venv/lib/python3.10/site-packages (from ipykernel) (1.8.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in ./.venv/lib/python3.10/site-packages (from ipykernel) (8.25.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./.venv/lib/python3.10/site-packages (from ipykernel) (8.6.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./.venv/lib/python3.10/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in ./.venv/lib/python3.10/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from ipykernel) (6.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in ./.venv/lib/python3.10/site-packages (from ipykernel) (26.0.3)\n",
      "Requirement already satisfied: tornado>=6.1 in ./.venv/lib/python3.10/site-packages (from ipykernel) (6.4.1)\n",
      "Requirement already satisfied: Mako in ./.venv/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow==2.14.1) (1.3.5)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in ./.venv/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow==2.14.1) (2.2.2)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in ./.venv/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi) (2.6.1)\n",
      "Requirement already satisfied: typer>=0.12.3 in ./.venv/lib/python3.10/site-packages (from fastapi-cli>=0.0.2->fastapi) (0.12.3)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in ./.venv/lib/python3.10/site-packages (from Flask<4->mlflow==2.14.1) (3.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in ./.venv/lib/python3.10/site-packages (from Flask<4->mlflow==2.14.1) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in ./.venv/lib/python3.10/site-packages (from Flask<4->mlflow==2.14.1) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./.venv/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow==2.14.1) (4.0.11)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in ./.venv/lib/python3.10/site-packages (from graphene<4->mlflow==2.14.1) (3.2.3)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in ./.venv/lib/python3.10/site-packages (from graphene<4->mlflow==2.14.1) (3.2.0)\n",
      "Requirement already satisfied: aniso8601<10,>=8 in ./.venv/lib/python3.10/site-packages (from graphene<4->mlflow==2.14.1) (9.0.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow==2.14.1) (3.19.2)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow==2.14.1) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat==5.10.4) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat==5.10.4) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat==5.10.4) (0.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./.venv/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat==5.10.4) (4.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.14.1) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.14.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.14.1) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.14.1) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.14.1) (3.1.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./.venv/lib/python3.10/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow==2.14.1) (1.2.14)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow==2.14.1) (0.46b0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas<3->mlflow==2.14.1) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow==2.14.1) (3.3.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.14.1) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.14.1) (3.5.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (12.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in ./.venv/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow==2.14.1) (1.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./.venv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow==2.14.1) (5.0.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (13.7.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow==2.14.1 openai  transformers torch torchvision evaluate datasets openai tiktoken fastapi rouge_score textstat tenacity plotly ipykernel nbformat==5.10.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in ./.venv/lib/python3.10/site-packages (5.10.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in ./.venv/lib/python3.10/site-packages (from nbformat) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in ./.venv/lib/python3.10/site-packages (from nbformat) (4.22.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.venv/lib/python3.10/site-packages (from nbformat) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in ./.venv/lib/python3.10/site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (0.18.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./.venv/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nbformat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Restart your jupyter kernel after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=<your-api-key>\n"
     ]
    }
   ],
   "source": [
    "%env OPENAI_API_KEY=<your-api-key>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to define your OPENAI API key as an enviroment variable to follow this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "# Run a quick validation that we have an entry for the OPEN_API_KEY within environment variables\n",
    "assert \"OPENAI_API_KEY\" in os.environ, \"OPENAI_API_KEY environment variable must be set\"\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by logging our translation model on mlflow.\n",
    "\n",
    "For this tutorial let's use a gpt3.5 with a system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = \"Translate the following sentences into Spanish\"\n",
    "basic_translation_model = mlflow.openai.log_model(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    task=openai.chat.completions,\n",
    "    artifact_path=\"model\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"{user_input}\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model to make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(basic_translation_model.model_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola, ¿cómo estás?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Set of Inputs to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Prepare evaluation data\n",
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        \"llm_inputs\": [\n",
    "            \"I'm over the moon about the news!\",\n",
    "            \"Spill the beans.\",\n",
    "            \"Bite the bullet.\",\n",
    "            \"Better late than never.\",\n",
    "            \n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here let's define some custom metric. \n",
    "For this use case we are going to translate some phrases from English to Spanish.\n",
    "\n",
    "We want to adress how faithfull the translation is, for that we have to consider cultural factors and not only a literal translation.\n",
    "\n",
    "Let's set a metric that takes into account that cultural sensitivity.\n",
    "\n",
    "Begin by providing a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Define the custom metric\n",
    "cultural_sensitivity = mlflow.metrics.genai.make_genai_metric(\n",
    "    name=\"cultural_sensitivity\",\n",
    "    definition=\"Assesses how well the translation preserves cultural nuances and idioms.\",\n",
    "    grading_prompt=\"Score from 1-5, where 1 is culturally insensitive and 5 is highly culturally aware.\",\n",
    "    examples=[\n",
    "        mlflow.metrics.genai.EvaluationExample(\n",
    "            input=\"Break a leg!\",\n",
    "            output=\"¡Rómpete una pierna!\",\n",
    "            score=2,\n",
    "            justification=\"This is a literal translation that doesn't capture the idiomatic meaning.\"\n",
    "        ),\n",
    "        mlflow.metrics.genai.EvaluationExample(\n",
    "            input=\"Break a leg!\",\n",
    "            output=\"¡Mucha mierda!\",\n",
    "            score=5,\n",
    "            justification=\"This translation uses the equivalent Spanish theater idiom, showing high cultural awareness.\"\n",
    "        ),\n",
    "        mlflow.metrics.genai.EvaluationExample(\n",
    "            input=\"It's raining cats and dogs.\",\n",
    "            output=\"Está lloviendo gatos y perros.\",\n",
    "            score=1,\n",
    "            justification=\"This literal translation does not convey the idiomatic meaning of heavy rain.\"\n",
    "        ),\n",
    "        mlflow.metrics.genai.EvaluationExample(\n",
    "            input=\"It's raining cats and dogs.\",\n",
    "            output=\"Está lloviendo a cántaros.\",\n",
    "            score=5,\n",
    "            justification=\"This translation uses a Spanish idiom that accurately conveys the meaning of heavy rain.\"\n",
    "        ),\n",
    "        mlflow.metrics.genai.EvaluationExample(\n",
    "            input=\"Kick the bucket.\",\n",
    "            output=\"Patear el balde.\",\n",
    "            score=1,\n",
    "            justification=\"This literal translation fails to convey the idiomatic meaning of dying.\"\n",
    "        ),\n",
    "        mlflow.metrics.genai.EvaluationExample(\n",
    "            input=\"Kick the bucket.\",\n",
    "            output=\"Estirar la pata.\",\n",
    "            score=5,\n",
    "            justification=\"This translation uses the equivalent Spanish idiom for dying, showing high cultural awareness.\"\n",
    "        ),\n",
    "        mlflow.metrics.genai.EvaluationExample(\n",
    "            input=\"Once in a blue moon.\",\n",
    "            output=\"Una vez en una luna azul.\",\n",
    "            score=2,\n",
    "            justification=\"This literal translation does not capture the rarity implied by the idiom.\"\n",
    "        ),\n",
    "        mlflow.metrics.genai.EvaluationExample(\n",
    "            input=\"Once in a blue moon.\",\n",
    "            output=\"De vez en cuando.\",\n",
    "            score=4,\n",
    "            justification=\"This translation captures the infrequency but lacks the idiomatic color of the original.\"\n",
    "        ),\n",
    "        mlflow.metrics.genai.EvaluationExample(\n",
    "            input=\"The ball is in your court.\",\n",
    "            output=\"La pelota está en tu cancha.\",\n",
    "            score=3,\n",
    "            justification=\"This translation is understandable but somewhat lacks the idiomatic nuance of making a decision.\"\n",
    "        ),\n",
    "        mlflow.metrics.genai.EvaluationExample(\n",
    "            input=\"The ball is in your court.\",\n",
    "            output=\"Te toca a ti.\",\n",
    "            score=5,\n",
    "            justification=\"This translation accurately conveys the idiomatic meaning of it being someone else's turn to act.\"\n",
    "        )\n",
    "    ],\n",
    "    model=\"openai:/gpt-4\",\n",
    "    parameters={\"temperature\": 0.0},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of this let's use mlflow default metrics for the evaluators. In this case mlflow wll use roberta-hate-speech model to detect the toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/26 09:14:09 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2024/06/26 09:14:10 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "/Users/azeveped/product-review-app/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()\n",
    "# 3. Log and evaluate the model\n",
    "with mlflow.start_run() as run:\n",
    "    results = mlflow.evaluate(\n",
    "        basic_translation_model.model_uri,\n",
    "        data=eval_data,\n",
    "        model_type=\"text\",\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[cultural_sensitivity],\n",
    "        evaluator_config={\n",
    "        \"col_mapping\": {\n",
    "            \"inputs\": \"llm_inputs\",\n",
    "           }}\n",
    "   )\n",
    "    \n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 636.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llm_inputs</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "      <th>toxicity/v1/score</th>\n",
       "      <th>flesch_kincaid_grade_level/v1/score</th>\n",
       "      <th>ari_grade_level/v1/score</th>\n",
       "      <th>cultural_sensitivity/v1/score</th>\n",
       "      <th>cultural_sensitivity/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm over the moon about the news!</td>\n",
       "      <td>¡Estoy feliz por la noticia!</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4</td>\n",
       "      <td>The translation captures the general sentiment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spill the beans.</td>\n",
       "      <td>Revela el secreto.</td>\n",
       "      <td>7</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5</td>\n",
       "      <td>The translation accurately captures the idioma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bite the bullet.</td>\n",
       "      <td>Aguanta el golpe.</td>\n",
       "      <td>7</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2</td>\n",
       "      <td>The translation \"Aguanta el golpe\" is a litera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Better late than never.</td>\n",
       "      <td>Más vale tarde que nunca.</td>\n",
       "      <td>7</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5</td>\n",
       "      <td>The translation accurately captures the idioma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          llm_inputs                       outputs  \\\n",
       "0  I'm over the moon about the news!  ¡Estoy feliz por la noticia!   \n",
       "1                   Spill the beans.            Revela el secreto.   \n",
       "2                   Bite the bullet.             Aguanta el golpe.   \n",
       "3            Better late than never.     Más vale tarde que nunca.   \n",
       "\n",
       "   token_count  toxicity/v1/score  flesch_kincaid_grade_level/v1/score  \\\n",
       "0            9           0.000258                                  5.2   \n",
       "1            7           0.001017                                  9.2   \n",
       "2            7           0.001586                                  0.9   \n",
       "3            7           0.004947                                  0.5   \n",
       "\n",
       "   ari_grade_level/v1/score  cultural_sensitivity/v1/score  \\\n",
       "0                       3.7                              4   \n",
       "1                       5.2                              5   \n",
       "2                       3.6                              2   \n",
       "3                       0.9                              5   \n",
       "\n",
       "               cultural_sensitivity/v1/justification  \n",
       "0  The translation captures the general sentiment...  \n",
       "1  The translation accurately captures the idioma...  \n",
       "2  The translation \"Aguanta el golpe\" is a litera...  \n",
       "3  The translation accurately captures the idioma...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results_table\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cultural Sensitivity Score: 4.0\n",
      "Pureness Score: 99.80%\n"
     ]
    }
   ],
   "source": [
    "cultural_sensitive_score = results.metrics['cultural_sensitivity/v1/mean']\n",
    "print(f\"Cultural Sensitivity Score: {cultural_sensitive_score}\")\n",
    "\n",
    "toxicity_score = results.metrics['toxicity/v1/mean'] \n",
    "# Calculate pureness score\n",
    "purity_score = \"{:.2f}\".format((1 - toxicity_score) * 100)\n",
    "print(f\"Pureness Score: {purity_score}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's good for stakeholders to have a visual representation of these metrics in a dashboard for monitoring quality assurance and so on. Le'ts build a Gauge we can later show to our stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "domain": {
          "x": [
           0,
           0.45
          ],
          "y": [
           0,
           1
          ]
         },
         "gauge": {
          "axis": {
           "range": [
            null,
            5
           ]
          }
         },
         "mode": "gauge+number",
         "title": {
          "text": "Cultural Sensitivity Score"
         },
         "type": "indicator",
         "value": 4
        },
        {
         "domain": {
          "x": [
           0.55,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "gauge": {
          "axis": {
           "range": [
            null,
            100
           ]
          }
         },
         "mode": "gauge+number",
         "title": {
          "text": "Purity Score (Non Toxicity)"
         },
         "type": "indicator",
         "value": 99.8
        }
       ],
       "layout": {
        "height": 400,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 800
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def create_gauge_chart(value1, title1, value2, title2):\n",
    "    # Create a subplot figure with two columns\n",
    "    fig = make_subplots(rows=1, cols=2, specs=[[{'type': 'indicator'}, {'type': 'indicator'}]])\n",
    "\n",
    "    # Add the first gauge chart\n",
    "    fig.add_trace(go.Indicator(\n",
    "        mode = \"gauge+number\",\n",
    "        value = value1,\n",
    "        title = {'text': title1},\n",
    "        gauge = {'axis': {'range': [None, 5]}}\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    # Add the second gauge chart\n",
    "    fig.add_trace(go.Indicator(\n",
    "        mode = \"gauge+number\",\n",
    "        value = value2,\n",
    "        title = {'text': title2},\n",
    "        gauge = {'axis': {'range': [None, 100]}}\n",
    "    ), row=1, col=2)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(height=400, width=800)\n",
    "\n",
    "    # Show figure\n",
    "    fig.show()\n",
    "\n",
    "create_gauge_chart(cultural_sensitive_score, \"Cultural Sensitivity Score\", float(purity_score), \"Purity Score (Non Toxicity)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Example - Faithfullness\n",
    "Let's say we want to evaluate how well our LLM is performing based on the context we provide it.\n",
    "For this lets define a custom metric called faithfullness\n",
    "\n",
    "For this example instead of passing an mlflow model let's pass in a custom function to the `mlflow evaluate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare evaluation data\n",
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        \"llm_inputs\": [\n",
    "            \"\"\"Question: What is the company's policy on employee training?\n",
    "context: \"Our company offers various training programs to support employee development. Employees are required to complete at least one training course per year related to their role. Additional training opportunities are available based on performance reviews.\" \"\"\",\n",
    "            \"\"\"Question: What is the company's policy on sick leave?\n",
    "context: \"Employees are entitled to 10 days of paid sick leave per year. Sick leave can be used for personal illness or to care for an immediate family member. A doctor's note is required for sick leave exceeding three consecutive days.\" \"\"\",\n",
    "            \"\"\"Question: How does the company handle performance reviews?\n",
    "context: \"Performance reviews are conducted annually. Employees are evaluated based on their job performance, goal achievement, and overall contribution to the team. Feedback is provided, and development plans are created to support employee growth.\" \"\"\",\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "        mlflow.metrics.genai.EvaluationExample(\n",
    "            input=\"\"\"Question: What is the company's policy on remote work?\n",
    "context: \"Our company supports a flexible working environment. Employees can work remotely up to three days a week, provided they maintain productivity and attend all mandatory meetings.\" \"\"\",\n",
    "            output=\"Employees can work remotely up to three days a week if they maintain productivity and attend mandatory meetings.\",\n",
    "            score=5,\n",
    "            justification=\"The answer is accurate and directly related to the question and context provided.\"\n",
    "        ),\n",
    "        mlflow.metrics.genai.EvaluationExample(\n",
    "            input=\"\"\"Question: What is the company's policy on remote work?\n",
    "context: \"Our company supports a flexible working environment. Employees can work remotely up to three days a week, provided they maintain productivity and attend all mandatory meetings.\" \"\"\",\n",
    "            output=\"Employees are allowed to work remotely as long as they want.\",\n",
    "            score=2,\n",
    "            justification=\"The answer is somewhat related but incorrect because it does not mention the three-day limit.\"\n",
    "        ),\n",
    "        mlflow.metrics.genai.EvaluationExample(\n",
    "            input=\"\"\"Question: What is the company's policy on remote work?\n",
    "context: \"Our company supports a flexible working environment. Employees can work remotely up to three days a week, provided they maintain productivity and attend all mandatory meetings.\" \"\"\",\n",
    "            output=\"Our company supports flexible work arrangements.\",\n",
    "            score=3,\n",
    "            justification=\"The answer is related to the context but does not specifically answer the question about the remote work policy.\"\n",
    "        ),\n",
    "        mlflow.metrics.genai.EvaluationExample(\n",
    "            input=\"\"\"Question: What is the company's annual leave policy?\n",
    "context: \"Employees are entitled to 20 days of paid annual leave per year. Leave must be approved by the employee's direct supervisor and should be planned in advance to ensure minimal disruption to work.\" \"\"\",\n",
    "            output=\"Employees are entitled to 20 days of paid annual leave per year, which must be approved by their supervisor.\",\n",
    "            score=5,\n",
    "            justification=\"The answer is accurate and directly related to the question and context provided.\"\n",
    "        )]\n",
    "\n",
    "# 1. Define the custom metric\n",
    "faithfulness = mlflow.metrics.genai.make_genai_metric(\n",
    "    name=\"faithfulness\",\n",
    "    definition=\"Assesses how well the answer relates to the question and provided context.\",\n",
    "    grading_prompt=\"Score from 1-5, where 1 is not related at all and 5 is highly relevant and accurate.\",\n",
    "    examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using custom function\n",
    "def my_llm(inputs):\n",
    "    answers = []\n",
    "    system_prompt = \"Please answer the following question in formal language based on the context provided.\"\n",
    "    for index, row in inputs.iterrows():\n",
    "        print('INPUTS:', row)\n",
    "        completion = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"{row}\"},\n",
    "            ],\n",
    "        )\n",
    "        answers.append(completion.choices[0].message.content)\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/26 09:14:47 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS: llm_inputs    Question: What is the company's policy on empl...\n",
      "Name: 0, dtype: object\n",
      "INPUTS: llm_inputs    Question: What is the company's policy on sick...\n",
      "Name: 1, dtype: object\n",
      "INPUTS: llm_inputs    Question: How does the company handle performa...\n",
      "Name: 2, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/26 09:14:52 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.84s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    results = mlflow.evaluate(\n",
    "        my_llm,\n",
    "        eval_data,\n",
    "        model_type=\"text\",\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[faithfulness],\n",
    "        evaluator_config={\n",
    "        \"col_mapping\": {\n",
    "            \"inputs\": \"llm_inputs\",\n",
    "           }}\n",
    "    )\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 555.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llm_inputs</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "      <th>toxicity/v1/score</th>\n",
       "      <th>flesch_kincaid_grade_level/v1/score</th>\n",
       "      <th>ari_grade_level/v1/score</th>\n",
       "      <th>faithfulness/v1/score</th>\n",
       "      <th>faithfulness/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question: What is the company's policy on empl...</td>\n",
       "      <td>The company's policy on employee benefits and ...</td>\n",
       "      <td>89</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>14.4</td>\n",
       "      <td>16.8</td>\n",
       "      <td>1</td>\n",
       "      <td>The model's output does not relate to the inpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question: What is the company's policy on sick...</td>\n",
       "      <td>I'm sorry, but the question appears to be cut ...</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1</td>\n",
       "      <td>The model's response is not related to the que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: How does the company handle performa...</td>\n",
       "      <td>The company handles performance evaluations by...</td>\n",
       "      <td>82</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>19.5</td>\n",
       "      <td>23.1</td>\n",
       "      <td>5</td>\n",
       "      <td>The model's output accurately reflects the con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          llm_inputs  \\\n",
       "0  Question: What is the company's policy on empl...   \n",
       "1  Question: What is the company's policy on sick...   \n",
       "2  Question: How does the company handle performa...   \n",
       "\n",
       "                                             outputs  token_count  \\\n",
       "0  The company's policy on employee benefits and ...           89   \n",
       "1  I'm sorry, but the question appears to be cut ...           35   \n",
       "2  The company handles performance evaluations by...           82   \n",
       "\n",
       "   toxicity/v1/score  flesch_kincaid_grade_level/v1/score  \\\n",
       "0           0.000175                                 14.4   \n",
       "1           0.000147                                  7.0   \n",
       "2           0.000165                                 19.5   \n",
       "\n",
       "   ari_grade_level/v1/score  faithfulness/v1/score  \\\n",
       "0                      16.8                      1   \n",
       "1                       7.4                      1   \n",
       "2                      23.1                      5   \n",
       "\n",
       "                       faithfulness/v1/justification  \n",
       "0  The model's output does not relate to the inpu...  \n",
       "1  The model's response is not related to the que...  \n",
       "2  The model's output accurately reflects the con...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results_table\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative we can also use the built in metric for faithfullness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=faithfulness, greater_is_better=True, long_name=faithfulness, version=v1, metric_details=\n",
      "Task:\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's faithfulness based on the rubric\n",
      "justification: Your reasoning about the model's faithfulness score\n",
      "\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called faithfulness based on the input and output.\n",
      "A definition of faithfulness and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Faithfulness is only evaluated with the provided output and provided context, please ignore the provided input entirely when scoring faithfulness. Faithfulness assesses how much of the provided output is factually consistent with the provided context. A higher score indicates that a higher proportion of claims present in the output can be derived from the provided context. Faithfulness does not consider how much extra information from the context is not present in the output.\n",
      "\n",
      "Grading rubric:\n",
      "Faithfulness: Below are the details for different scores:\n",
      "- Score 1: None of the claims in the output can be inferred from the provided context.\n",
      "- Score 2: Some of the claims in the output can be inferred from the provided context, but the majority of the output is missing from, inconsistent with, or contradictory to the provided context.\n",
      "- Score 3: Half or more of the claims in the output can be inferred from the provided context.\n",
      "- Score 4: Most of the claims in the output can be inferred from the provided context, with very little information that is not directly supported by the provided context.\n",
      "- Score 5: All of the claims in the output are directly supported by the provided context, demonstrating high faithfulness to the provided context.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example Output:\n",
      "Employees can work remotely up to three days a week if they maintain productivity and attend mandatory meetings.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: context\n",
      "value:\n",
      "None\n",
      "\n",
      "Example score: 5\n",
      "Example justification: The answer is accurate and directly related to the question and context provided.\n",
      "        \n",
      "\n",
      "Example Output:\n",
      "Employees are allowed to work remotely as long as they want.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: context\n",
      "value:\n",
      "None\n",
      "\n",
      "Example score: 2\n",
      "Example justification: The answer is somewhat related but incorrect because it does not mention the three-day limit.\n",
      "        \n",
      "\n",
      "Example Output:\n",
      "Our company supports flexible work arrangements.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: context\n",
      "value:\n",
      "None\n",
      "\n",
      "Example score: 3\n",
      "Example justification: The answer is related to the context but does not specifically answer the question about the remote work policy.\n",
      "        \n",
      "\n",
      "Example Output:\n",
      "Employees are entitled to 20 days of paid annual leave per year, which must be approved by their supervisor.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: context\n",
      "value:\n",
      "None\n",
      "\n",
      "Example score: 5\n",
      "Example justification: The answer is accurate and directly related to the question and context provided.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's faithfulness based on the rubric\n",
      "justification: Your reasoning about the model's faithfulness score\n",
      "\n",
      "Do not add additional new lines. Do not add any other fields.\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import faithfulness\n",
    "faithfullness_metric = faithfulness(model=\"openai:/gpt-4\",examples=examples)\n",
    "print(faithfullness_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric works quite well in singery with langchain retrivals since you can provide the grading context seperatly from the llm_input column if you prefer.\n",
    "\n",
    "Since in this example we are doing everything in the same input column let's map out the context column to our input column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/26 09:14:59 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS: llm_inputs    Question: What is the company's policy on empl...\n",
      "Name: 0, dtype: object\n",
      "INPUTS: llm_inputs    Question: What is the company's policy on sick...\n",
      "Name: 1, dtype: object\n",
      "INPUTS: llm_inputs    Question: How does the company handle performa...\n",
      "Name: 2, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/26 09:15:04 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.43s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()\n",
    "with mlflow.start_run() as run:\n",
    "    results = mlflow.evaluate(\n",
    "        my_llm,\n",
    "        eval_data,\n",
    "        model_type=\"text\",\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[faithfullness_metric],\n",
    "        evaluator_config={\n",
    "        \"col_mapping\": {\n",
    "            \"inputs\": \"llm_inputs\",\n",
    "            \"context\": \"llm_inputs\",\n",
    "           }}\n",
    "    )\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfullness Sensitivity Score: 4.0\n",
      "Pureness Score: 99.99\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "domain": {
          "x": [
           0,
           0.45
          ],
          "y": [
           0,
           1
          ]
         },
         "gauge": {
          "axis": {
           "range": [
            null,
            5
           ]
          }
         },
         "mode": "gauge+number",
         "title": {
          "text": "Faithfulness Score"
         },
         "type": "indicator",
         "value": 4
        },
        {
         "domain": {
          "x": [
           0.55,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "gauge": {
          "axis": {
           "range": [
            null,
            100
           ]
          }
         },
         "mode": "gauge+number",
         "title": {
          "text": "Purity Score (Non Toxicity)"
         },
         "type": "indicator",
         "value": 99.99
        }
       ],
       "layout": {
        "height": 400,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 800
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "faithfullness_sensitive_score = results.metrics['faithfulness/v1/mean']\n",
    "print(f\"Faithfullness Sensitivity Score: {cultural_sensitive_score}\")\n",
    "\n",
    "toxicity_score = results.metrics['toxicity/v1/mean']\n",
    "# Calculate pureness score\n",
    "purity_score = \"{:.2f}\".format((1 - toxicity_score) * 100)\n",
    "print(f\"Pureness Score: {purity_score}\")\n",
    "create_gauge_chart(cultural_sensitive_score, \"Faithfulness Score\", float(purity_score), \"Purity Score (Non Toxicity)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
